# ğŸ¤ Samanvay â€” A Two-Way Sign Language Interpreter

> *Bridging the gap between signed and spoken language through AI-powered real-time interpretation.*

![React](https://img.shields.io/badge/React-18-61DAFB?logo=react)
![TypeScript](https://img.shields.io/badge/TypeScript-5.6-3178C6?logo=typescript)
![TensorFlow.js](https://img.shields.io/badge/TensorFlow.js-4.22-FF6F00?logo=tensorflow)
![MediaPipe](https://img.shields.io/badge/MediaPipe-Tasks_Vision-4285F4?logo=google)
![Vite](https://img.shields.io/badge/Vite-6-646CFF?logo=vite)
![License](https://img.shields.io/badge/License-MIT-green)

<img width="1920" height="1080" alt="Samanvay â€” Sign Language Interpreter" src="https://github.com/user-attachments/assets/32dad445-9377-4dfb-8396-246d597222be" />

---

## ğŸ¯ Vision

**Samanvay** (à¤¸à¤®à¤¨à¥à¤µà¤¯ â€” meaning *harmony, coordination*) was born from a personal journey of wanting to learn Indian Sign Language and realizing the vast communication gap between the hearing and deaf communities.

This is not a toy demo â€” it's a **modular, production-ready architecture** for a two-way sign language interpreter that runs **entirely in the browser**. A trained Dense neural network classifies hand landmarks extracted by MediaPipe into 29 sign classes in real-time. The system is designed so the model can be swapped with CNN/LSTM architectures without touching the rest of the codebase.

---

## âœ¨ Features

### Two-Way Communication

| ğŸ–ï¸ Sign â†’ Text | âœï¸ Text â†’ Sign |
|---|---|
| Real-time webcam hand tracking | Text & voice input |
| ML-powered gesture recognition (29 classes) | Animated sign avatar display |
| Sentence building from detected signs | Word-by-word sign sequence |
| Text-to-Speech output | Fingerspelling fallback |
| Emotion-aware tone modification | â€” |

### Core Capabilities

- **ğŸ§  ML Sign Classification** â€” Dense neural network trained on 29K+ hand landmark samples (ASL Alphabet dataset), exported to TensorFlow.js for browser inference
- **ğŸ–ï¸ Hand Landmark Detection** â€” MediaPipe HandLandmarker with 21-point tracking and GPU acceleration
- **ğŸ˜Š Emotion Detection** â€” Facial expression analysis (Happy, Sad, Neutral, Angry, Surprised) via face landmarks
- **ğŸ“ Teach Me Mode** â€” Interactive learning with keypoint visualization, confidence scores, and real-time feedback
- **ğŸ”Š Speech Integration** â€” Text-to-Speech & Speech-to-Text via Web Speech API
- **ğŸŒ Multilingual Architecture** â€” Translation abstraction layer ready for Hindi and beyond

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        App Shell                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚   Header     â”‚  â”‚ Mode Toggle  â”‚  â”‚  Teach Me Toggle    â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚     Left Panel         â”‚         Right Panel                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  WebcamFeed /     â”‚  â”‚  â”‚  OutputPanel / SignAvatar     â”‚  â”‚
â”‚  â”‚  TextInput        â”‚  â”‚  â”‚                               â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                     Engine Layer                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ ModelLoader  â”‚  â”‚ EmotionDetectâ”‚  â”‚ TranslationService  â”‚ â”‚
â”‚  â”‚ + Gesture   â”‚  â”‚              â”‚  â”‚ + TextToSign        â”‚ â”‚
â”‚  â”‚ + Dictionaryâ”‚  â”‚              â”‚  â”‚                     â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚         â”‚                â”‚                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ MediaPipe   â”‚  â”‚ MediaPipe    â”‚  â”‚  TensorFlow.js      â”‚ â”‚
â”‚  â”‚ HandLandmarkâ”‚  â”‚ FaceLandmark â”‚  â”‚  Sign Classifier    â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  State: Zustand    Speech: Web Speech API    Styles: CSS     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Project Structure

```
Samanvay/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ engine/          # GestureEngine, ModelLoader, LandmarkProcessor, SignDictionary
â”‚   â”œâ”€â”€ emotion/         # Facial emotion classification from face landmarks
â”‚   â”œâ”€â”€ translation/     # Abstraction layer for multi-language support
â”‚   â”œâ”€â”€ speech/          # Text-to-Speech and Speech-to-Text wrappers
â”‚   â”œâ”€â”€ store/           # Zustand-based global state management
â”‚   â”œâ”€â”€ hooks/           # useWebcam, useMediaPipe, useTeachMe
â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”œâ”€â”€ Layout/      # Header, ModeSelector, SplitScreen
â”‚   â”‚   â”œâ”€â”€ Camera/      # WebcamFeed with live hand tracking
â”‚   â”‚   â”œâ”€â”€ Chat/        # OutputPanel, SentenceDisplay, TextInput
â”‚   â”‚   â”œâ”€â”€ Avatar/      # SignAvatar for sign animation
â”‚   â”‚   â””â”€â”€ TeachMe/     # ConfidenceScore, interactive learning UI
â”‚   â””â”€â”€ styles/          # CSS variables, animations, global styles
â”œâ”€â”€ training/
â”‚   â”œâ”€â”€ train_sign_model.py   # ML training pipeline (MediaPipe â†’ Dense NN â†’ TF.js)
â”‚   â”œâ”€â”€ collect_custom.py     # Custom sign data collection tool
â”‚   â””â”€â”€ requirements.txt
â””â”€â”€ public/models/            # Trained TF.js model (auto-loaded by app)
```

---

## ğŸš€ Getting Started

### Prerequisites

- **Node.js 18+**
- **Python 3.10+** (only for model training)
- A modern browser with WebGL support (Chrome recommended)
- Webcam (for Sign â†’ Text mode)

### Quick Start

```bash
# Clone the repository
git clone https://github.com/okayniti/Samanvay.git
cd Samanvay

# Install dependencies
npm install

# Start development server
npm run dev
```

Open **http://localhost:5173** in Chrome. The pre-trained model loads automatically â€” start signing! ğŸ¤Ÿ

### Production Build

```bash
npm run build
npm run preview
```

---

## ğŸ§  Model Training (Optional)

The app ships with a pre-trained model. If you want to retrain or customize:

```bash
cd training
python -m venv venv

# Activate virtual environment
# Windows (Git Bash): source venv/Scripts/activate
# macOS/Linux:        source venv/bin/activate

# Install dependencies
pip install opencv-python mediapipe tensorflow numpy scikit-learn Pillow tqdm kagglehub tf-keras matplotlib

# Run training pipeline
python train_sign_model.py
```

### Pipeline Overview

| Step | What Happens |
|---|---|
| **1. Data** | Downloads the ASL Alphabet dataset (87K images, 29 classes) via Kaggle API |
| **2. Extraction** | MediaPipe extracts 21 hand landmarks (63 features) per image |
| **3. Training** | Dense neural network trains on normalized landmark vectors |
| **4. Export** | Model exports to TensorFlow.js format â†’ `public/models/sign_classifier/` |

**First run** takes ~15â€“30 min for extraction (cached for future runs). Training takes ~2 min.

---

## ğŸ› ï¸ Tech Stack

| Layer | Technology |
|---|---|
| **Frontend** | React 18 + TypeScript |
| **Build Tool** | Vite 6 |
| **State Management** | Zustand |
| **Hand/Face Detection** | MediaPipe Tasks Vision |
| **Sign Classification** | TensorFlow.js (Dense NN) |
| **Speech** | Web Speech API (SpeechSynthesis + SpeechRecognition) |
| **Styling** | Vanilla CSS with custom design tokens |
| **ML Training** | TensorFlow + scikit-learn + MediaPipe (Python) |

---

## ğŸ’¡ Why I Built This

Learning sign language opened my eyes to how isolated deaf and hard-of-hearing people can feel in everyday interactions. Existing tools are either:

- **Research prototypes** locked behind academic paywalls
- **Toy demos** that recognize a handful of ASL letters
- **ASL-only** â€” ignoring Indian Sign Language entirely

Samanvay is my attempt at building something **real**: a modular, extensible platform that respects sign language as a first-class language, with an architecture that grows from heuristic matching to full neural models.

---

## ğŸ”® Roadmap

| Feature | Status |
|---|---|
| Dense NN sign classifier (29 classes) | âœ… Complete |
| TF.js browser inference pipeline | âœ… Complete |
| Two-way communication (Sign â†” Text) | âœ… Complete |
| Teach Me learning mode | âœ… Complete |
| Emotion-aware speech output | âœ… Complete |
| CNN/LSTM sequence model | ğŸ—ï¸ Architecture ready |
| 3D avatar for sign animation | ğŸ—ï¸ Plug-in point created |
| Hindi language support | ğŸ—ï¸ Translation layer abstracted |
| ISL-specific gesture dataset | ğŸ“‹ Planned |
| Mobile responsive PWA | ğŸ“‹ Planned |
| Analytics dashboard | ğŸ“‹ Planned |

---

## ğŸ“„ License

MIT Â© [Niti](https://github.com/okayniti)

---

<p align="center">
  <strong>Samanvay</strong> â€” Because communication is a human right, not a privilege. ğŸ¤Ÿ
</p>
